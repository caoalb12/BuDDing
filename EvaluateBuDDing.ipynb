{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bd02dCw1O97"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DistilBertTokenizer\n",
        "import torch\n",
        "import torch.autograd.profiler as profiler\n",
        "import random, time\n",
        "import json\n",
        "import torch\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('winogrande.json', 'r') as f:\n",
        "    winogrande_data = json.load(f)\n",
        "\n",
        "with open('copa.json', 'r') as f:\n",
        "    copa_data = json.load(f)\n",
        "\n",
        "with open('hellaswag.json', 'r') as f:\n",
        "    hellaswag_data = json.load(f)\n",
        "\n",
        "samples_from_each = 256\n",
        "\n",
        "winogrande_entries = winogrande_data[:samples_from_each]\n",
        "copa_entries = copa_data[:samples_from_each]\n",
        "hellaswag_entries = hellaswag_data[:samples_from_each]\n",
        "\n",
        "combined_dataset = copa_entries + winogrande_entries + hellaswag_entries\n",
        "\n",
        "combined_prompts = []\n",
        "combined_answers = []\n",
        "for example in combined_dataset:\n",
        "    combined_answers.append(example[\"answer\"])\n",
        "\n",
        "    for option in example[\"options\"]:\n",
        "        combined_prompts.append(option)\n",
        "\n",
        "print('Combined dataset created.')\n",
        "print(f'Total entries: {len(combined_dataset)}')"
      ],
      "metadata": {
        "id": "4t_UwUuQeWJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "omission_sets = [\n",
        "  (4, 16, 18, 19, 20, 22),\n",
        "  (4, 13, 17, 18, 20, 21),\n",
        "  (4, 12, 14, 17, 20, 21)\n",
        "]\n",
        "\n",
        "class DistilBERTLossPredictor(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(DistilBERTLossPredictor, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.regressor = nn.Linear(self.distilbert.config.hidden_size, 3)  # 3 outputs\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        dropped = self.dropout(cls_output)\n",
        "        predictions = self.regressor(dropped)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "def load_router_model(model_path=\"best_model.pt\"):\n",
        "    model = DistilBERTLossPredictor().to(device)\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Loaded model trained until epoch {checkpoint['epoch']}, \"f\"val_loss={checkpoint['val_loss']:.4f}.\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_skip_layers(prompts, router_model, outer_tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    For each prompt, use the router to predict losses for each omission set\n",
        "    and select the omission set with minimum predicted loss.\n",
        "\n",
        "    Returns:\n",
        "        skip_layers: List[List[int]] of length len(prompts)\n",
        "        chosen_indices: List[int] router-chosen omission-set index (0,1,2) per prompt\n",
        "        predicted_losses: torch.Tensor of shape (len(prompts), 3)\n",
        "    \"\"\"\n",
        "    print(f\"Generating skip layers. Number of prompts: {len(prompts)}.\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    router_model.eval()\n",
        "\n",
        "    # Batch tokenize prompts\n",
        "    enc = router_tokenizer(\n",
        "        prompts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # shape: [batch_size, 3] -> losses for [os1, os2, os3]\n",
        "        predicted_losses = router_model(input_ids, attention_mask)\n",
        "        noise_scale = 1 # try 0.01 â†’ 0.1 depending on how strong you want the variation\n",
        "        torch.manual_seed(45)  # Or your chosen seed\n",
        "        predicted_losses = predicted_losses + noise_scale * torch.randn_like(predicted_losses)\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    chosen_indices = predicted_losses.argmin(dim=-1)\n",
        "    chosen_indices_cpu = chosen_indices.cpu().numpy()\n",
        "    skip_layers = [omission_sets[idx] for idx in chosen_indices_cpu]\n",
        "\n",
        "    t2 = time.time()\n",
        "\n",
        "    frequencies = Counter(skip_layers)\n",
        "    print(f\"Omission set distribution: {frequencies}.\")\n",
        "\n",
        "    return skip_layers\n",
        "\n",
        "router_model, router_tokenizer = load_router_model()"
      ],
      "metadata": {
        "id": "1nKNF3Rg1VSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4sb-srJTj3DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sequence_nll(log_softmax, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    Compute per-sequence negative log likelihood (average per token)\n",
        "    for a batch of sequences.\n",
        "\n",
        "    Args:\n",
        "        log_softmax: (batch, seq, vocab) log-probs\n",
        "        input_ids:   (batch, seq)\n",
        "        attention_mask: (batch, seq)\n",
        "\n",
        "    Returns:\n",
        "        List[float]: NLL per sequence\n",
        "    \"\"\"\n",
        "def compute_sequence_nll(logits, input_ids, attention_mask):\n",
        "    # logits: [batch, seq, vocab]\n",
        "    # input_ids, attention_mask: [batch, seq]\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    # shift input_ids for next-token prediction\n",
        "    target_ids = input_ids[:, 1:]\n",
        "    log_probs = log_probs[:, :-1, :]\n",
        "    # gather log probs of correct tokens\n",
        "    token_log_probs = log_probs.gather(2, target_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    # mask padding tokens\n",
        "    mask = attention_mask[:, 1:]\n",
        "    nll = -(token_log_probs * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "    nll_list = nll.cpu().tolist()\n",
        "\n",
        "    del log_probs, target_ids, token_log_probs, mask, nll\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return nll_list"
      ],
      "metadata": {
        "id": "dFv90MaHr4kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_model_forward_pass(prompts):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Get omission sets for each prompt\n",
        "    skip_layers = generate_skip_layers(prompts, router_model, router_tokenizer)\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    # Tokenize all prompts\n",
        "    tokenizer_output = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = tokenizer_output.input_ids.to(device)\n",
        "    attention_mask = tokenizer_output.attention_mask.to(device)\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get embeddings\n",
        "        hidden_states = model.transformer.wte(input_ids)\n",
        "\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=device).unsqueeze(0)\n",
        "        position_embeds = model.transformer.wpe(position_ids)\n",
        "        hidden_states = hidden_states + position_embeds\n",
        "\n",
        "        seq_len = attention_mask.shape[1]\n",
        "\n",
        "        # Create attention masks\n",
        "        causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).view(1, 1, seq_len, seq_len)\n",
        "        attention_mask_4d = attention_mask.view(batch_size, 1, 1, seq_len)\n",
        "        combined_mask = causal_mask * attention_mask_4d\n",
        "        combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float16).min\n",
        "\n",
        "        # Forward pass through transformer layers\n",
        "        for layer_idx, layer in enumerate(model.transformer.h):\n",
        "            start_time = time.time()\n",
        "            active_indices = [i for i in range(batch_size) if layer_idx not in skip_layers[i]]\n",
        "\n",
        "            if not active_indices:\n",
        "                continue\n",
        "\n",
        "            mini_batch = hidden_states[active_indices]\n",
        "            mini_batch_mask = combined_mask[active_indices]\n",
        "\n",
        "            mini_batch = layer(mini_batch, attention_mask=mini_batch_mask)[0]\n",
        "            hidden_states[active_indices] = mini_batch\n",
        "\n",
        "            end_time = time.time()\n",
        "            print(f\"Layer {layer_idx}: Prompts = {len(active_indices)}, Forward Pass Time: {end_time - start_time:.4} sec\")\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        hidden_states = model.transformer.ln_f(hidden_states)\n",
        "        logits = model.lm_head(hidden_states)\n",
        "\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Calculate log probabilities for each token in each prompt\n",
        "        log_softmax = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "        nll_losses = compute_sequence_nll(log_softmax, input_ids, attention_mask)\n",
        "\n",
        "        print(f\"Router: {t1 - t0:.4f}s | Forward: {t2 - t1:.4f}s | E2E: {t2 - t0:.4f}s\")\n",
        "\n",
        "        del hidden_states, logits, log_softmax\n",
        "\n",
        "    return nll_losses"
      ],
      "metadata": {
        "id": "3PhIypzA1YL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_no_batched_model_forward_pass(prompts):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Get omission sets for each prompt (router can stay batched)\n",
        "    skip_layers = generate_skip_layers(prompts, router_model, router_tokenizer)\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    # Tokenize all prompts once; we'll slice per-prompt to get batch_size=1\n",
        "    tokenizer_output = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    all_input_ids = tokenizer_output.input_ids.to(device)          # [B, S_max]\n",
        "    all_attention_mask = tokenizer_output.attention_mask.to(device)  # [B, S_max]\n",
        "\n",
        "    nll_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Loop over prompts; each forward pass uses batch_size = 1\n",
        "        for i in range(len(prompts)):\n",
        "            prompt_start_time = time.time()\n",
        "\n",
        "            # Slice out a single prompt: shapes [1, S]\n",
        "            input_ids = all_input_ids[i:i+1]\n",
        "            attention_mask = all_attention_mask[i:i+1]\n",
        "\n",
        "            # Embeddings\n",
        "            hidden_states = model.transformer.wte(input_ids)\n",
        "            seq_len = input_ids.shape[1]\n",
        "\n",
        "            # Positional embeddings (match seq_len of this prompt)\n",
        "            position_ids = torch.arange(seq_len, device=device).unsqueeze(0)  # [1, S]\n",
        "            position_embeds = model.transformer.wpe(position_ids)\n",
        "            hidden_states = hidden_states + position_embeds\n",
        "\n",
        "            # Attention masks for this single sequence (batch_size = 1)\n",
        "            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device))\n",
        "            causal_mask = causal_mask.view(1, 1, seq_len, seq_len)          # [1, 1, S, S]\n",
        "\n",
        "            attention_mask_4d = attention_mask.view(1, 1, 1, seq_len)       # [1, 1, 1, S]\n",
        "            combined_mask = causal_mask * attention_mask_4d\n",
        "            combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float16).min\n",
        "\n",
        "            # Forward pass through transformer layers with dynamic pruning\n",
        "            for layer_idx, layer in enumerate(model.transformer.h):\n",
        "                if layer_idx in skip_layers[i]:\n",
        "                    # This layer is pruned for this prompt\n",
        "                    continue\n",
        "\n",
        "                layer_start = time.time()\n",
        "                hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "                layer_end = time.time()\n",
        "\n",
        "                print(\n",
        "                    f\"Prompt {i}, Layer {layer_idx}: \"\n",
        "                    f\"Forward Pass Time: {layer_end - layer_start:.4f} sec\"\n",
        "                )\n",
        "\n",
        "            # Final layer norm and projection to logits\n",
        "            hidden_states = model.transformer.ln_f(hidden_states)\n",
        "            logits = model.lm_head(hidden_states)  # [1, S, V]\n",
        "\n",
        "            # Compute NLL for this single sequence; returns [one_value]\n",
        "            nll_list = compute_sequence_nll(logits, input_ids, attention_mask)\n",
        "            nll_losses.extend(nll_list)\n",
        "\n",
        "            prompt_end = time.time()\n",
        "            print(\n",
        "                f\"Prompt {i}: Total Forward Time (bs=1) = \"\n",
        "                f\"{prompt_end - prompt_start_time:.4f} sec\"\n",
        "            )\n",
        "\n",
        "            # Cleanup per-prompt to be safe\n",
        "            del input_ids, attention_mask, hidden_states, logits, nll_list\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        t2 = time.time()\n",
        "\n",
        "    print(f\"Router: {t1 - t0:.4f}s | Forward (all prompts, bs=1): {t2 - t1:.4f}s | E2E: {t2 - t0:.4f}s\")\n",
        "\n",
        "    return nll_losses"
      ],
      "metadata": {
        "id": "mGqYMFpoitdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_model_forward_pass(prompts):\n",
        "    \"\"\"\n",
        "    Run the model normally (all layers, no skipping) on a batch of prompts\n",
        "    and compute per-sequence NLLs.\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Tokenize\n",
        "    tokenizer_output = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = tokenizer_output.input_ids.to(device)\n",
        "    attention_mask = tokenizer_output.attention_mask.to(device)\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        # Compute log probabilities and NLL\n",
        "        log_softmax = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "        nll_losses = compute_sequence_nll(log_softmax, input_ids, attention_mask)\n",
        "\n",
        "        t2 = time.time()\n",
        "\n",
        "        print(f\"E2E: {t1 - t0:.4f}s\")\n",
        "\n",
        "        del logits, log_softmax, input_ids, attention_mask\n",
        "        torch.cuda.empty_cache()  # optionally clear GPU memory\n",
        "\n",
        "        return nll_losses"
      ],
      "metadata": {
        "id": "JNNch061rdoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_skip_model_forward_pass(prompts):\n",
        "    \"\"\"\n",
        "    Optimized forward pass with dynamic layer skipping and parallel execution.\n",
        "    Reduces overhead by pre-computing active indices and reusing streams.\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    skip_layers = generate_skip_layers(prompts, router_model, router_tokenizer)\n",
        "    t1 = time.time()\n",
        "\n",
        "    # Tokenize and prepare inputs\n",
        "    tokenizer_output = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = tokenizer_output.input_ids.to(device)\n",
        "    attention_mask = tokenizer_output.attention_mask.to(device)\n",
        "\n",
        "    # Configuration\n",
        "    PARALLEL_PAIRS = {17: 18, 21: 22}\n",
        "    processed_layers = set()\n",
        "\n",
        "    # Reuse CUDA streams (creating streams has overhead)\n",
        "    stream1 = torch.cuda.Stream()\n",
        "    stream2 = torch.cuda.Stream()\n",
        "\n",
        "    batch_size = len(prompts)  # Fixed: was combined_prompts\n",
        "    layers_needed = [set(range(24)) - set(skip_layers[i]) for i in range(batch_size)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Initialize embeddings\n",
        "        hidden_states = model.transformer.wte(input_ids)\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=device).unsqueeze(0)\n",
        "        hidden_states += model.transformer.wpe(position_ids)\n",
        "\n",
        "        # Prepare attention mask (compute once)\n",
        "        seq_len = attention_mask.shape[1]\n",
        "        causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).view(1, 1, seq_len, seq_len)\n",
        "        combined_mask = causal_mask * attention_mask.view(batch_size, 1, 1, seq_len)\n",
        "        combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float16).min\n",
        "\n",
        "        # Process layers\n",
        "        for layer_idx, layer in enumerate(model.transformer.h):\n",
        "            if layer_idx in processed_layers:\n",
        "                continue\n",
        "\n",
        "            if layer_idx in PARALLEL_PAIRS:\n",
        "                next_layer_idx = PARALLEL_PAIRS[layer_idx]\n",
        "\n",
        "                # NOTE: need_1 and need_2 will be mutually exclusive by design\n",
        "                need_1 = [i for i in range(batch_size) if layer_idx in layers_needed[i]]\n",
        "                need_2 = [i for i in range(batch_size) if next_layer_idx in layers_needed[i]]\n",
        "\n",
        "                with torch.cuda.stream(stream1):\n",
        "                    out_1 = model.transformer.h[layer_idx](\n",
        "                        hidden_states[need_1].clone(),\n",
        "                        attention_mask=combined_mask[need_1]\n",
        "                    )[0]\n",
        "\n",
        "                with torch.cuda.stream(stream2):\n",
        "                    out_2 = model.transformer.h[next_layer_idx](\n",
        "                        hidden_states[need_2].clone(),\n",
        "                        attention_mask=combined_mask[need_2]\n",
        "                    )[0]\n",
        "\n",
        "                stream1.synchronize()\n",
        "                stream2.synchronize()\n",
        "\n",
        "                hidden_states[need_1] = out_1\n",
        "                hidden_states[need_2] = out_2\n",
        "\n",
        "                processed_layers.add(next_layer_idx)\n",
        "            else:\n",
        "                # Fast lookup using pre-computed sets\n",
        "                active = [i for i in range(batch_size) if layer_idx in layers_needed[i]]\n",
        "\n",
        "                if active:\n",
        "                    hidden_states[active] = layer(\n",
        "                        hidden_states[active],\n",
        "                        attention_mask=combined_mask[active]\n",
        "                    )[0]\n",
        "\n",
        "        # Final projection\n",
        "        hidden_states = model.transformer.ln_f(hidden_states)\n",
        "        logits = model.lm_head(hidden_states)\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Compute NLL\n",
        "        nll_losses = compute_sequence_nll(\n",
        "            torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "            input_ids, attention_mask\n",
        "        )\n",
        "\n",
        "        print(f\"Router: {t1 - t0:.4f}s | Forward: {t2 - t1:.4f}s | E2E: {t2 - t0:.4f}s\")\n",
        "\n",
        "        del hidden_states, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return nll_losses"
      ],
      "metadata": {
        "id": "SGOdaBdQ9PcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(combined_answers, nll_losses):\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for idx in range(0, len(nll_losses) - 1, 2):\n",
        "      loss1 = nll_losses[idx]\n",
        "      loss2 = nll_losses[idx + 1]\n",
        "\n",
        "      prediction = 0 if loss1 < loss2 else 1\n",
        "      correct_answer = combined_answers[idx//2]\n",
        "\n",
        "      correct_predictions += int(prediction == correct_answer)\n",
        "\n",
        "  print(f\"Accuracy: {correct_predictions}/{len(combined_answers)}.\")"
      ],
      "metadata": {
        "id": "6OOqrB26m98a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Profiling batching WITH dynamic pruning.\")\n",
        "\n",
        "skip_nll_losses = skip_model_forward_pass(combined_prompts)\n",
        "evaluate_performance(combined_answers, skip_nll_losses)"
      ],
      "metadata": {
        "id": "z75zsc8lm5wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Profiling NO batching with dynamic pruning.\")\n",
        "\n",
        "skip_nll_losses = skip_no_batched_model_forward_pass(combined_prompts)\n",
        "evaluate_performance(combined_answers, skip_nll_losses)"
      ],
      "metadata": {
        "id": "-uqIQARciYX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Profiling batching WITHOUT dynamic pruning.\")\n",
        "\n",
        "standard_nll_losses = standard_model_forward_pass(combined_prompts)\n",
        "evaluate_performance(combined_answers, standard_nll_losses)"
      ],
      "metadata": {
        "id": "y2zd4K-Dww1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Profiling PARALLELIZED batching WITH dynamic pruning.\")\n",
        "\n",
        "parallel_nll_losses = parallel_skip_model_forward_pass(combined_prompts)\n",
        "evaluate_performance(combined_answers, parallel_nll_losses)"
      ],
      "metadata": {
        "id": "IelkF7WE29fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
