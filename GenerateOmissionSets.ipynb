{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFVnbMlD9f7_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcbR4MH_9tXR"
      },
      "outputs": [],
      "source": [
        "def forward_with_skipped_layers(model, input_ids, attention_mask, skip_layers):\n",
        "    \"\"\"\n",
        "    Performs a forward pass through GPT-Neo while skipping the transformer\n",
        "    layers specified in skip_layers.\n",
        "    \"\"\"\n",
        "    # Embeddings\n",
        "    hidden_states = model.transformer.wte(input_ids)\n",
        "    position_ids = torch.arange(input_ids.shape[1], device=device).unsqueeze(0)\n",
        "    hidden_states = hidden_states + model.transformer.wpe(position_ids)\n",
        "\n",
        "    seq_len = attention_mask.shape[1]\n",
        "    batch_size = input_ids.shape[0]\n",
        "\n",
        "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).view(\n",
        "        1, 1, seq_len, seq_len\n",
        "    )\n",
        "    attention_mask_4d = attention_mask.view(batch_size, 1, 1, seq_len)\n",
        "    combined_mask = causal_mask * attention_mask_4d\n",
        "    combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float16).min\n",
        "\n",
        "    # Transformer layers\n",
        "    for idx, layer in enumerate(model.transformer.h):\n",
        "        if idx in skip_layers:\n",
        "            continue\n",
        "        hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "    # Final layer norm + LM head\n",
        "    hidden_states = model.transformer.ln_f(hidden_states)\n",
        "    logits = model.lm_head(hidden_states)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIl_PC-l9ryv"
      },
      "outputs": [],
      "source": [
        "def winogrande_prompt(sentence, option):\n",
        "    \"\"\"\n",
        "    Insert the option into the blank (_) in the Winogrande sentence.\n",
        "    \"\"\"\n",
        "    return sentence.replace(\"_\", option)\n",
        "\n",
        "\n",
        "def evaluate_winogrande(model, tokenizer, skip_layers, sample_size=256):\n",
        "    \"\"\"\n",
        "    Evaluates GPT-Neo on Winogrande using perplexity comparison.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"winogrande\", \"winogrande_xl\", split=\"validation\")\n",
        "    dataset = dataset.select(range(sample_size))\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for example in dataset:\n",
        "        sentence = example[\"sentence\"]\n",
        "        option1 = example[\"option1\"]\n",
        "        option2 = example[\"option2\"]\n",
        "        label = example[\"answer\"]  # \"1\" or \"2\"\n",
        "\n",
        "        # Make two filled-in sentences\n",
        "        s1 = winogrande_prompt(sentence, option1)\n",
        "        s2 = winogrande_prompt(sentence, option2)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs1 = tokenizer(s1, return_tensors=\"pt\").to(device)\n",
        "        inputs2 = tokenizer(s2, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits1 = forward_with_skipped_layers(\n",
        "                model, inputs1.input_ids, inputs1.attention_mask, skip_layers\n",
        "            )\n",
        "            logits2 = forward_with_skipped_layers(\n",
        "                model, inputs2.input_ids, inputs2.attention_mask, skip_layers\n",
        "            )\n",
        "\n",
        "        # Compute sentence losses (negative log-likelihood)\n",
        "        def compute_loss(logits, labels):\n",
        "            shift_logits = logits[:, :-1].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                shift_labels.view(-1),\n",
        "                reduction=\"mean\",\n",
        "            )\n",
        "            return loss.item()\n",
        "\n",
        "        loss1 = compute_loss(logits1, inputs1.input_ids)\n",
        "        loss2 = compute_loss(logits2, inputs2.input_ids)\n",
        "\n",
        "        # Model chooses the option with *lower* loss\n",
        "        pred = \"1\" if loss1 < loss2 else \"2\"\n",
        "        # print()\n",
        "        # print(f\"Option 1: {s1}\")\n",
        "        # print(f\"Option 2: {s2}\")\n",
        "        # print(f\"Option {label} is correct. The model found option {pred} more likely.\")\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / sample_size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5pPYrx59lSi"
      },
      "outputs": [],
      "source": [
        "def hellaswag_prompt(context, ending):\n",
        "    \"\"\"\n",
        "    Combine context + ending for HellaSwag.\n",
        "    \"\"\"\n",
        "    # You can customize how you join context + ending; simplest:\n",
        "    return context + \" \" + ending\n",
        "\n",
        "\n",
        "def evaluate_hellaswag(model, tokenizer, skip_layers, sample_size=256):\n",
        "    \"\"\"\n",
        "    Evaluates GPT-Neo on HellaSwag using perplexity comparison.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"hellaswag\", split=\"validation\")\n",
        "    dataset = dataset.select(range(sample_size))\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for example in dataset:\n",
        "        context = example[\"ctx\"]\n",
        "        endings = example[\"endings\"]  # list of 4 endings\n",
        "        label = int(example[\"label\"])     # integer 0..3\n",
        "\n",
        "        losses = []\n",
        "        for ending in endings:\n",
        "            sentence = hellaswag_prompt(context, ending)\n",
        "            # print(sentence)\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = forward_with_skipped_layers(\n",
        "                    model, inputs.input_ids, inputs.attention_mask, skip_layers\n",
        "                )\n",
        "\n",
        "            # Compute negative log-likelihood\n",
        "            shift_logits = logits[:, :-1].contiguous()\n",
        "            shift_labels = inputs.input_ids[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                shift_labels.view(-1),\n",
        "                reduction=\"mean\",\n",
        "            )\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # Model chooses the ending with the lowest loss\n",
        "        pred = int(torch.argmin(torch.tensor(losses)))\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / sample_size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF5KHT4T9lMi"
      },
      "outputs": [],
      "source": [
        "def boolq_prompt(passage, question, answer_text):\n",
        "    \"\"\"\n",
        "    Combine passage + question + answer candidate as a prompt.\n",
        "    We'll score the model's likelihood of the answer_text being correct.\n",
        "    \"\"\"\n",
        "    # Simple prompt template\n",
        "    return f\"Passage: {passage}\\nQuestion: {question}\\nAnswer: {answer_text}\"\n",
        "\n",
        "def evaluate_boolq(model, tokenizer, skip_layers=[], sample_size=256):\n",
        "    \"\"\"\n",
        "    Evaluate GPT-Neo on BoolQ using likelihood comparison.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"boolq\", split=\"validation\")\n",
        "    dataset = dataset.select(range(sample_size))\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for example in dataset:\n",
        "        passage = example[\"passage\"]\n",
        "        question = example[\"question\"]\n",
        "        label = example[\"answer\"]  # True/False\n",
        "\n",
        "        # Candidate options\n",
        "        candidates = [\"True\", \"False\"]\n",
        "\n",
        "        losses = []\n",
        "        for option in candidates:\n",
        "            prompt = boolq_prompt(passage, question, option)\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = forward_with_skipped_layers(\n",
        "                    model, inputs.input_ids, inputs.attention_mask, skip_layers\n",
        "                )\n",
        "\n",
        "            # Compute negative log-likelihood\n",
        "            shift_logits = logits[:, :-1].contiguous()\n",
        "            shift_labels = inputs.input_ids[:, 1:].contiguous()\n",
        "            mask = shift_labels != tokenizer.pad_token_id\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, shift_logits.size(-1))[mask.view(-1)],\n",
        "                shift_labels.view(-1)[mask.view(-1)],\n",
        "                reduction=\"mean\",\n",
        "            )\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # Model predicts the option with the lowest loss\n",
        "        pred_idx = int(torch.argmin(torch.tensor(losses)))\n",
        "        pred = True if candidates[pred_idx] == \"True\" else False\n",
        "\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / sample_size\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2N3hVP59lBf"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def copa_prompt(premise, choice, question_type):\n",
        "    \"\"\"\n",
        "    Build COPA prompt.\n",
        "    question_type is either 'cause' or 'effect'.\n",
        "    \"\"\"\n",
        "    if question_type == \"cause\":\n",
        "        return f\"{premise} This happened because {choice}.\"\n",
        "    else:  # effect\n",
        "        return f\"{premise} As a result, {choice}.\"\n",
        "\n",
        "\n",
        "def evaluate_copa(model, tokenizer, skip_layers, sample_size=200):\n",
        "    \"\"\"\n",
        "    Evaluates COPA using loss comparison between choice1 and choice2.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"pkavumba/balanced-copa\", split=\"train\")\n",
        "    dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for example in dataset:\n",
        "        premise = example[\"premise\"]\n",
        "        choice1 = example[\"choice1\"]\n",
        "        choice2 = example[\"choice2\"]\n",
        "        qtype   = example[\"question\"]    # \"cause\" or \"effect\"\n",
        "        label   = int(example[\"label\"])  # 0 or 1\n",
        "\n",
        "        s1 = copa_prompt(premise, choice1, qtype)\n",
        "        s2 = copa_prompt(premise, choice2, qtype)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs1 = tokenizer(s1, return_tensors=\"pt\").to(model.device)\n",
        "        inputs2 = tokenizer(s2, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits1 = forward_with_skipped_layers(\n",
        "                model, inputs1.input_ids, inputs1.attention_mask, skip_layers\n",
        "            )\n",
        "            logits2 = forward_with_skipped_layers(\n",
        "                model, inputs2.input_ids, inputs2.attention_mask, skip_layers\n",
        "            )\n",
        "\n",
        "        # NLL losses\n",
        "        def loss_fn(logits, labels):\n",
        "            shift_logits = logits[:, :-1].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            return F.cross_entropy(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                shift_labels.view(-1),\n",
        "                reduction=\"mean\"\n",
        "            ).item()\n",
        "\n",
        "        loss1 = loss_fn(logits1, inputs1.input_ids)\n",
        "        loss2 = loss_fn(logits2, inputs2.input_ids)\n",
        "\n",
        "        pred = 0 if loss1 < loss2 else 1\n",
        "\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdlX9_ZX9lHF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def greedy_layer_pruning(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    evaluate_fn,        # function: (model, tokenizer, skip_layers, sample_size) -> accuracy\n",
        "    max_layers_to_remove=6,\n",
        "    num_samples=128,\n",
        "    output_file=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Greedy layer pruning: iteratively remove layers that hurt accuracy the least.\n",
        "\n",
        "    Args:\n",
        "        model: HuggingFace transformer model\n",
        "        tokenizer: corresponding tokenizer\n",
        "        evaluate_fn: evaluation function that returns accuracy\n",
        "        num_layers: number of transformer layers; inferred from model if None\n",
        "        max_layers_to_remove: max number of layers to prune\n",
        "        num_samples: number of dataset samples to use in evaluation\n",
        "        output_file: JSON filename to save full log\n",
        "    \"\"\"\n",
        "    num_layers = len(model.transformer.h)\n",
        "\n",
        "    removed_layers = set()\n",
        "    removal_history = []\n",
        "    full_test_log = []\n",
        "\n",
        "    # Baseline accuracy\n",
        "    baseline_acc = evaluate_fn(model, tokenizer, skip_layers=[], sample_size=num_samples)\n",
        "    print(f\"Baseline accuracy: {baseline_acc:.2%}\")\n",
        "\n",
        "    removal_history.append({\n",
        "        'iteration': 0,\n",
        "        'removed_layers': [],\n",
        "        'accuracy': baseline_acc\n",
        "    })\n",
        "\n",
        "    for iteration in range(1, max_layers_to_remove + 1):\n",
        "        print(f\"\\nIteration {iteration}: finding best layer to remove...\")\n",
        "        best_acc = -1.0\n",
        "        best_layer = None\n",
        "\n",
        "        candidate_layers = [l for l in range(num_layers) if l not in removed_layers]\n",
        "        iteration_log = []\n",
        "\n",
        "        for layer in tqdm(candidate_layers):\n",
        "            test_skip_layers = removed_layers | {layer}\n",
        "            acc = evaluate_fn(model, tokenizer, skip_layers=list(test_skip_layers), sample_size=num_samples)\n",
        "\n",
        "            # Log every layer tested\n",
        "            iteration_log.append({\n",
        "                'tested_layer': layer,\n",
        "                'skip_layers': sorted(list(test_skip_layers)),\n",
        "                'accuracy': acc\n",
        "            })\n",
        "\n",
        "            # Pick the layer whose removal hurts accuracy the least\n",
        "            if acc >= best_acc:\n",
        "                best_acc = acc\n",
        "                best_layer = layer\n",
        "\n",
        "        # Permanently remove the best layer\n",
        "        removed_layers.add(best_layer)\n",
        "        removal_history.append({\n",
        "            'iteration': iteration,\n",
        "            'layer_removed': best_layer,\n",
        "            'removed_layers': sorted(list(removed_layers)),\n",
        "            'accuracy': best_acc\n",
        "        })\n",
        "\n",
        "        # Append iteration log to full log\n",
        "        full_test_log.append({\n",
        "            'iteration': iteration,\n",
        "            'tested_candidates': iteration_log,\n",
        "            'selected_layer': best_layer,\n",
        "            'accuracy_after_removal': best_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Removed layer {best_layer}. New removed set: {sorted(list(removed_layers))}\")\n",
        "        print(f\"Accuracy after removal: {best_acc:.2%}\")\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\nGreedy Layer Pruning Summary:\")\n",
        "    for record in removal_history:\n",
        "        if record['iteration'] == 0:\n",
        "            print(f\"Baseline: Accuracy={record['accuracy']:.2%}\")\n",
        "        else:\n",
        "            print(f\"After removing {record['iteration']} layer(s) ({record['removed_layers']}): Accuracy={record['accuracy']:.2%}\")\n",
        "\n",
        "    # Save full log if requested\n",
        "    if output_file is not None:\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(full_test_log, f, indent=4)\n",
        "\n",
        "    return removal_history, full_test_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FS-0f3L9jrq"
      },
      "outputs": [],
      "source": [
        "samples = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NdFF8CB19g1U"
      },
      "outputs": [],
      "source": [
        "hellaswag_history, hellaswag_log = greedy_layer_pruning(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    evaluate_fn=evaluate_hellaswag,\n",
        "    max_layers_to_remove=6,\n",
        "    num_samples=samples,\n",
        "    output_file=\"hellaswag_pruning_log.json\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD7FXmgU9guz"
      },
      "outputs": [],
      "source": [
        "winogrande_history, winogrande_log = greedy_layer_pruning(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    evaluate_fn=evaluate_winogrande,\n",
        "    max_layers_to_remove=6,\n",
        "    num_samples=samples,\n",
        "    output_file=\"winogrande_pruning_log.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr2JurJX9gpN"
      },
      "outputs": [],
      "source": [
        "boolq_history, boolq_log = greedy_layer_pruning(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    evaluate_fn=evaluate_boolq,\n",
        "    max_layers_to_remove=6,\n",
        "    num_samples=samples,\n",
        "    output_file=\"boolq_pruning_log.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FayQL6i19giT"
      },
      "outputs": [],
      "source": [
        "copa_history, copa_log = greedy_layer_pruning(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    evaluate_fn=evaluate_copa,\n",
        "    max_layers_to_remove=6,\n",
        "    num_samples=samples,\n",
        "    output_file=\"copa_pruning_log.json\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
