{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765,
     "referenced_widgets": [
      "203851a25ce340dc9d5fcb63a3a26603",
      "111513fa1182416485b4e0c3d1fd0734",
      "315941b225ad4cd3b17bdc47767ec455",
      "de389c271195448abe6c3e993a2196e9",
      "24868c6d5e4f4ff580a120a3c37754f3",
      "c1c69948bcac4fcaaa968990e2a38598",
      "821d12cce10947bcbb06aef289404d2f",
      "1408a60170384cfa9bf61fd9634adc13",
      "dcb15bd9a3454196b973318ca7586388",
      "bbb54b947af84cc5b8d388ece59d1912",
      "278f528c1c5a473db51b99956c2f13b1",
      "463bd48410974070a742460324d91adf",
      "c48bac13b21c4a1f9003ddbef3f1ba1a",
      "6cb1dd6364534de2b5b9a303b26d0207",
      "fc410a684b7544e1907724d70794a466",
      "2747df63565643668b79d8df618b37f4",
      "6e6189633e0e4dfd8a504130674e0bfa",
      "d819834e631b4b60a7ab509c683f9883",
      "044442a5b78f409c9084c801006294eb",
      "2a6141c82b094ec994285b23fac8b6ba",
      "bc7819e05c4042b39470ec1830db403d",
      "64b8f54f72f746a98e5585b03d343c01",
      "c91e2511cbdc4061a7156e00474a9fe0",
      "086d77a6b9fc437abcdfb7ade51237ad",
      "eac2020586824b5cb3d086f46d7ab395",
      "ac8b7e8645c5429da59eb1d2c0e02708",
      "5ef43c865a0c48318d2011fabfa58528",
      "df6bed6eea69427a9d1d52c9741f9530",
      "3ed4dcf7d49e4e99ad42fc70fe2cc53a",
      "d65d8db787f74d079e381a292f8546f5",
      "68808ea5358e47f7b5b70cd57883eac2",
      "2f79bea31f12493a804faf21cb6acd12",
      "a26ee1d1e5344a90a7e81e50bd6244d0",
      "1de378824e7a4dc8927e3b8695072ad4",
      "149f9f237a0341818d879e042ba6115d",
      "54c21c72eaf54a159fd398da30052be9",
      "8ec603b9a4e74355afe4fc5bdf40574a",
      "a8b21e6cfaf1466a82d94d67112feb7b",
      "ad75bf95ae16471b8363b971219625b5",
      "92fe343ee1f9481cb97ab5ff11815f05",
      "754125d08ddb4c4bac4ff828cc8e4eff",
      "139298c5dd434cdc8f0ac4471e81d7e6",
      "f119b63ce152461d8099226d6da02f5a",
      "c5a2214dfd1b4392bbd0b616673b30fd",
      "f344df9204834c72b585ddff3afee147",
      "24529ae3a6934e4d9b031b93dd3abbf1",
      "dcc1ba8a955045358a3bd45a68436613",
      "000d121c9b9c4bd08187bcf38c345a8f",
      "bd4ef4d701a84522a1a85fe2fface3e3",
      "7d7a570d348a47a29f15beb462c27beb",
      "104886ffd11b4d56b67694c66b4e9ac2",
      "8e7f7d7fced24491ac6c533563eb5608",
      "708c8b0bd5c84a5c83a5ad0af010a603",
      "f8da24c80a44479faf2868ef853f0e28",
      "065eb14756ed49b18a026ad83b1dd134",
      "bca1d59a456e46549b40a5fb2ce245e6",
      "789f4f9c379643d9a9643ea533b3396a",
      "f79e02f7946c4f39b07b6170cc1ed233",
      "c24c62c41fb4444c811f171386df83d9",
      "7d60fc2e90264eecacef04d9e8c5e720",
      "c9e3a86c764c4b1e9818bb41db280f89",
      "9af0215e38c34c9ab727efc6c371d770",
      "a8b98064e4844912b022a1b6ee29b087",
      "3753fb0bfadd4c5ba780f85ad4d6fc33",
      "b911860d6e45420db715623ac2fd989b",
      "098a4d1ec05b4d94a0f8e37058947d33"
     ]
    },
    "executionInfo": {
     "elapsed": 307686,
     "status": "ok",
     "timestamp": 1764534109023,
     "user": {
      "displayName": "Vishnu Kannan",
      "userId": "15922470962910517786"
     },
     "user_tz": 480
    },
    "id": "LZ6wjj6ZbXq0",
    "outputId": "3b6c582e-4bdd-49ec-bd5e-841bd18cbb0c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203851a25ce340dc9d5fcb63a3a26603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463bd48410974070a742460324d91adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91e2511cbdc4061a7156e00474a9fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de378824e7a4dc8927e3b8695072ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f344df9204834c72b585ddff3afee147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca1d59a456e46549b40a5fb2ce245e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "error",
     "timestamp": 1764533798957,
     "user": {
      "displayName": "Vishnu Kannan",
      "userId": "15922470962910517786"
     },
     "user_tz": 480
    },
    "id": "9Zdu1cgtcCPj",
    "outputId": "1a3f68f6-5a8c-424c-8ea5-fa26e3a4d96d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1721361770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mwino_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"winogrande\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"winogrande_xl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mhellaswag_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hellaswag\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcopa_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pkavumba/balanced-copa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# less than 10000 examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "omission_sets = [\n",
    "    [4, 16, 18, 19, 20, 22],\n",
    "    [4, 13, 17, 18, 20, 21],\n",
    "    [4, 12, 14, 17, 20, 21]\n",
    "]\n",
    "\n",
    "num_samples = 10000\n",
    "\n",
    "wino_ds = load_dataset(\"winogrande\", \"winogrande_xl\", split=\"train\")\n",
    "hellaswag_ds = load_dataset(\"hellaswag\", split=\"train\")\n",
    "copa_ds = load_dataset(\"pkavumba/balanced-copa\", split=\"train\") # less than 10000 examples\n",
    "\n",
    "print(wino_ds)\n",
    "print(hellaswag_ds)\n",
    "print(copa_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "error",
     "timestamp": 1764533795737,
     "user": {
      "displayName": "Vishnu Kannan",
      "userId": "15922470962910517786"
     },
     "user_tz": 480
    },
    "id": "3uSinffVcMRG",
    "outputId": "4f7b10ef-418d-4985-949d-173f1faaf885"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copa_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3567834661.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcopa_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcopa_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"premise\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choice1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copa_ds' is not defined"
     ]
    }
   ],
   "source": [
    "copa_examples = []\n",
    "\n",
    "for ex in copa_ds:\n",
    "    premise = ex[\"premise\"]\n",
    "    c1 = ex[\"choice1\"]\n",
    "    c2 = ex[\"choice2\"]\n",
    "    label = ex[\"label\"]\n",
    "    qtype = ex[\"question\"]\n",
    "\n",
    "    correct = c1 if label == 0 else c2\n",
    "\n",
    "    if qtype == \"cause\":\n",
    "        question = \"What was the cause?\"\n",
    "    else:\n",
    "        question = \"What was the effect?\"\n",
    "\n",
    "    prompt = f\"{premise}\\n{question}\"\n",
    "    copa_examples.append((\"Copa\", prompt, correct))\n",
    "\n",
    "for example in copa_examples[0:25]:\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "error",
     "timestamp": 1764533743823,
     "user": {
      "displayName": "Vishnu Kannan",
      "userId": "15922470962910517786"
     },
     "user_tz": 480
    },
    "id": "rY-itRPucyiB",
    "outputId": "76a76f7e-4640-4c72-e9c0-461a7b63b7f1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wino_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-10612023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwino_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwino_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moption1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"option1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wino_ds' is not defined"
     ]
    }
   ],
   "source": [
    "wino_examples = []\n",
    "\n",
    "for ex in wino_ds:\n",
    "    sentence = ex[\"sentence\"]\n",
    "    option1 = ex[\"option1\"]\n",
    "    option2 = ex[\"option2\"]\n",
    "    label = ex[\"answer\"]\n",
    "\n",
    "    correct = option1 if label == \"1\" else option2\n",
    "\n",
    "    # Replace blank with visible marker\n",
    "    masked_sentence = sentence.replace(\"_\", \"_____\")\n",
    "\n",
    "    question = \"What word best fills in the blank?\"\n",
    "\n",
    "    prompt = f\"{masked_sentence}\\n{question}\"\n",
    "    wino_examples.append((\"Winogrande\", prompt, correct))\n",
    "\n",
    "\n",
    "for example in wino_examples[0:25]:\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "11DH2vyuofqLZ3FIEV_VCSEH280n3Vw7G"
    },
    "id": "WP25nb2pcyRX",
    "outputId": "a8260808-8cce-41d9-bbda-f2980ce662e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hellaswag_examples = []\n",
    "\n",
    "for ex in hellaswag_ds:\n",
    "    print(ex)\n",
    "    context_a = ex[\"ctx_a\"]\n",
    "    context_b = ex[\"ctx_b\"]\n",
    "    if context_b:\n",
    "      context_b = context_b[0].upper() + context_b[1:]\n",
    "    label = ex[\"label\"]\n",
    "    correct = ex[\"endings\"][int(label)]\n",
    "\n",
    "    prompt = f\"{context_a}\\n{context_b}\"\n",
    "    hellaswag_examples.append((\"HellaSwag\", prompt, correct))\n",
    "\n",
    "for example in hellaswag_examples[0:25]:\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JS-t1-Hc3Pl"
   },
   "outputs": [],
   "source": [
    "def forward_with_skipped_layers(model, input_ids, attention_mask, skip_layers):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through GPT-Neo while skipping the transformer\n",
    "    layers specified in skip_layers.\n",
    "    \"\"\"\n",
    "    # Embeddings\n",
    "    hidden_states = model.transformer.wte(input_ids)\n",
    "    position_ids = torch.arange(input_ids.shape[1], device=device).unsqueeze(0)\n",
    "    hidden_states = hidden_states + model.transformer.wpe(position_ids)\n",
    "\n",
    "    seq_len = attention_mask.shape[1]\n",
    "    batch_size = input_ids.shape[0]\n",
    "\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).view(\n",
    "        1, 1, seq_len, seq_len\n",
    "    )\n",
    "    attention_mask_4d = attention_mask.view(batch_size, 1, 1, seq_len)\n",
    "    combined_mask = causal_mask * attention_mask_4d\n",
    "    combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float16).min\n",
    "\n",
    "    # Transformer layers\n",
    "    for idx, layer in enumerate(model.transformer.h):\n",
    "        if idx in skip_layers:\n",
    "            continue\n",
    "        hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
    "\n",
    "    # Final layer norm + LM head\n",
    "    hidden_states = model.transformer.ln_f(hidden_states)\n",
    "    logits = model.lm_head(hidden_states)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0xG-OOcc5Df"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def compute_answer_loss(prompt, answer, skip_layers):\n",
    "    \"\"\"\n",
    "    Compute the loss for generating the answer given the prompt,\n",
    "    with specified layers skipped using forward_with_skipped_layers.\n",
    "    \"\"\"\n",
    "    # tokenize prompt and answer together\n",
    "    full_text = prompt + \" \" + answer\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # tokenize just the prompt to find where answer starts\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    prompt_length = prompt_inputs.input_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # use forward_with_skipped_layers to get logits\n",
    "        logits = forward_with_skipped_layers(\n",
    "            model,\n",
    "            inputs.input_ids,\n",
    "            inputs.attention_mask,\n",
    "            skip_layers\n",
    "        )\n",
    "\n",
    "        # shift logits and labels for language modeling\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs.input_ids[:, 1:].contiguous()\n",
    "\n",
    "        # create mask for answer tokens only\n",
    "        answer_mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
    "        answer_mask[:, prompt_length-1:] = True\n",
    "\n",
    "        # compute loss only on answer tokens\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        losses = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        losses = losses.view(shift_labels.shape)\n",
    "\n",
    "        # average loss over answer tokens\n",
    "        answer_losses = losses[answer_mask]\n",
    "        if len(answer_losses) > 0:\n",
    "            avg_loss = answer_losses.mean().item()\n",
    "        else:\n",
    "            avg_loss = float('inf')\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1417665,
     "status": "ok",
     "timestamp": 1764185632851,
     "user": {
      "displayName": "Vishnu Kannan",
      "userId": "15922470962910517786"
     },
     "user_tz": 480
    },
    "id": "LjytzLgBhNDd",
    "outputId": "98367768-0a9c-465a-a2a7-a6298a7e4f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses and writing JSONL dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [23:37<00:00, 14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router training dataset saved to: router_training_data.jsonl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random, json\n",
    "\n",
    "print(\"Computing losses and writing JSONL dataset...\")\n",
    "output_file = \"router_training_data.jsonl\"\n",
    "\n",
    "examples = []\n",
    "\n",
    "examples.extend(copa_examples)\n",
    "examples.extend(wino_examples)\n",
    "examples.extend(hellaswag_examples)\n",
    "\n",
    "random.shuffle(examples)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for idx, (dataset_name, prompt, answer) in enumerate(tqdm(examples)):\n",
    "\n",
    "        record = {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "\n",
    "        for i, skip_layers in enumerate(omission_sets):\n",
    "            loss = compute_answer_loss(prompt, answer, skip_layers)\n",
    "            record[f\"os{i+1}_loss\"] = loss\n",
    "\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            f.flush()\n",
    "\n",
    "print(f\"Router training dataset saved to: {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN5rUfwDMV8dbLynHAO60bq",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
 }, 
 "nbformat": 4,
 "nbformat_minor": 0
}
