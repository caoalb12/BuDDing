{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ6wjj6ZbXq0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zdu1cgtcCPj"
      },
      "outputs": [],
      "source": [
        "omission_sets = [\n",
        "    [4, 16, 18, 19, 20, 22],\n",
        "    [4, 13, 17, 18, 20, 21],\n",
        "    [4, 12, 14, 17, 20, 21]\n",
        "]\n",
        "\n",
        "num_samples = 10000\n",
        "\n",
        "wino_ds = load_dataset(\"winogrande\", \"winogrande_xl\", split=\"train\")\n",
        "hellaswag_ds = load_dataset(\"hellaswag\", split=\"train\")\n",
        "copa_ds = load_dataset(\"pkavumba/balanced-copa\", split=\"train\") # less than 10000 examples\n",
        "\n",
        "print(wino_ds)\n",
        "print(hellaswag_ds)\n",
        "print(copa_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uSinffVcMRG"
      },
      "outputs": [],
      "source": [
        "copa_examples = []\n",
        "\n",
        "for ex in copa_ds:\n",
        "    premise = ex[\"premise\"]\n",
        "    c1 = ex[\"choice1\"]\n",
        "    c2 = ex[\"choice2\"]\n",
        "    label = ex[\"label\"]\n",
        "    qtype = ex[\"question\"]\n",
        "\n",
        "    correct = c1 if label == 0 else c2\n",
        "\n",
        "    if qtype == \"cause\":\n",
        "        question = \"What was the cause?\"\n",
        "    else:\n",
        "        question = \"What was the effect?\"\n",
        "\n",
        "    prompt = f\"{premise}\\n{question}\"\n",
        "    copa_examples.append((\"Copa\", prompt, correct))\n",
        "\n",
        "for example in copa_examples[0:25]:\n",
        "  print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY-itRPucyiB"
      },
      "outputs": [],
      "source": [
        "wino_examples = []\n",
        "\n",
        "for ex in wino_ds:\n",
        "    sentence = ex[\"sentence\"]\n",
        "    option1 = ex[\"option1\"]\n",
        "    option2 = ex[\"option2\"]\n",
        "    label = ex[\"answer\"]\n",
        "\n",
        "    correct = option1 if label == \"1\" else option2\n",
        "\n",
        "    # Replace blank with visible marker\n",
        "    masked_sentence = sentence.replace(\"_\", \"_____\")\n",
        "\n",
        "    question = \"What word best fills in the blank?\"\n",
        "\n",
        "    prompt = f\"{masked_sentence}\\n{question}\"\n",
        "    wino_examples.append((\"Winogrande\", prompt, correct))\n",
        "\n",
        "\n",
        "for example in wino_examples[0:25]:\n",
        "  print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WP25nb2pcyRX"
      },
      "outputs": [],
      "source": [
        "hellaswag_examples = []\n",
        "\n",
        "for ex in hellaswag_ds:\n",
        "    print(ex)\n",
        "    context_a = ex[\"ctx_a\"]\n",
        "    context_b = ex[\"ctx_b\"]\n",
        "    if context_b:\n",
        "      context_b = context_b[0].upper() + context_b[1:]\n",
        "    label = ex[\"label\"]\n",
        "    correct = ex[\"endings\"][int(label)]\n",
        "\n",
        "    prompt = f\"{context_a}\\n{context_b}\"\n",
        "    hellaswag_examples.append((\"HellaSwag\", prompt, correct))\n",
        "\n",
        "for example in hellaswag_examples[0:25]:\n",
        "  print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JS-t1-Hc3Pl"
      },
      "outputs": [],
      "source": [
        "def forward_with_skipped_layers(model, input_ids, attention_mask, skip_layers):\n",
        "    \"\"\"\n",
        "    Performs a forward pass through GPT-Neo while skipping the transformer\n",
        "    layers specified in skip_layers.\n",
        "    \"\"\"\n",
        "    # Embeddings\n",
        "    hidden_states = model.transformer.wte(input_ids)\n",
        "    position_ids = torch.arange(input_ids.shape[1], device=device).unsqueeze(0)\n",
        "    hidden_states = hidden_states + model.transformer.wpe(position_ids)\n",
        "\n",
        "    seq_len = attention_mask.shape[1]\n",
        "    batch_size = input_ids.shape[0]\n",
        "\n",
        "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).view(\n",
        "        1, 1, seq_len, seq_len\n",
        "    )\n",
        "    attention_mask_4d = attention_mask.view(batch_size, 1, 1, seq_len)\n",
        "    combined_mask = causal_mask * attention_mask_4d\n",
        "    combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float16).min\n",
        "\n",
        "    # Transformer layers\n",
        "    for idx, layer in enumerate(model.transformer.h):\n",
        "        if idx in skip_layers:\n",
        "            continue\n",
        "        hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "    # Final layer norm + LM head\n",
        "    hidden_states = model.transformer.ln_f(hidden_states)\n",
        "    logits = model.lm_head(hidden_states)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0xG-OOcc5Df"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def compute_answer_loss(prompt, answer, skip_layers):\n",
        "    \"\"\"\n",
        "    Compute the loss for generating the answer given the prompt,\n",
        "    with specified layers skipped using forward_with_skipped_layers.\n",
        "    \"\"\"\n",
        "    # tokenize prompt and answer together\n",
        "    full_text = prompt + \" \" + answer\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    # tokenize just the prompt to find where answer starts\n",
        "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
        "    prompt_length = prompt_inputs.input_ids.shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # use forward_with_skipped_layers to get logits\n",
        "        logits = forward_with_skipped_layers(\n",
        "            model,\n",
        "            inputs.input_ids,\n",
        "            inputs.attention_mask,\n",
        "            skip_layers\n",
        "        )\n",
        "\n",
        "        # shift logits and labels for language modeling\n",
        "        shift_logits = logits[:, :-1, :].contiguous()\n",
        "        shift_labels = inputs.input_ids[:, 1:].contiguous()\n",
        "\n",
        "        # create mask for answer tokens only\n",
        "        answer_mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
        "        answer_mask[:, prompt_length-1:] = True\n",
        "\n",
        "        # compute loss only on answer tokens\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "        losses = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1)\n",
        "        )\n",
        "        losses = losses.view(shift_labels.shape)\n",
        "\n",
        "        # average loss over answer tokens\n",
        "        answer_losses = losses[answer_mask]\n",
        "        if len(answer_losses) > 0:\n",
        "            avg_loss = answer_losses.mean().item()\n",
        "        else:\n",
        "            avg_loss = float('inf')\n",
        "\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjytzLgBhNDd"
      },
      "outputs": [],
      "source": [
        "import random, json\n",
        "\n",
        "print(\"Computing losses and writing JSONL dataset...\")\n",
        "output_file = \"router_training_data.jsonl\"\n",
        "\n",
        "examples = []\n",
        "\n",
        "examples.extend(copa_examples)\n",
        "examples.extend(wino_examples)\n",
        "examples.extend(hellaswag_examples)\n",
        "\n",
        "random.shuffle(examples)\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    for idx, (dataset_name, prompt, answer) in enumerate(tqdm(examples)):\n",
        "\n",
        "        record = {\n",
        "            \"dataset\": dataset_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"answer\": answer\n",
        "        }\n",
        "\n",
        "        for i, skip_layers in enumerate(omission_sets):\n",
        "            loss = compute_answer_loss(prompt, answer, skip_layers)\n",
        "            record[f\"os{i+1}_loss\"] = loss\n",
        "\n",
        "        f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            f.flush()\n",
        "\n",
        "print(f\"Router training dataset saved to: {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVp8LaaDMiVD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "checkpoint = torch.load('best_model.pt', map_location='cpu')\n",
        "\n",
        "# 3. Load model state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# 4. If you want, load optimizer state\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# 5. Access other info\n",
        "epoch = checkpoint['epoch']\n",
        "val_loss = checkpoint['val_loss']\n",
        "\n",
        "# 6. Set model to eval mode if using for inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded model from epoch {epoch} with validation loss {val_loss}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
